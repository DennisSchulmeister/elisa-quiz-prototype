#================================================================================
# Example for a local .env file. Use the following environment
# variables to configure the server.
#
# Note, that ALMOST all settings except the LLM API Key have the default values
# seen below. In your real configuration you only need to set the variable with
# deliberately different values.
#================================================================================

#--------------------------------------------------------------------------------
# General server settings:
#--------------------------------------------------------------------------------

# Network interface to listen on (usually 127.0.0.1 or 0.0.0.0)
ELISA_LISTEN_IP = 127.0.0.1

# Port number to listen on
ELISA_LISTEN_PORT = 8000

# Restart server on code changes (for development only)
ELISA_HOT_RELOAD = false

#--------------------------------------------------------------------------------
# Database
#--------------------------------------------------------------------------------

# Mongo connection URL
ELISA_MONGODB_URL = mongodb://username:password@localhost:27017/?compressors=zlib,snappy

#--------------------------------------------------------------------------------
# Language Model
#--------------------------------------------------------------------------------

# Provider and name of the used chat model
ELISA_LLM_CHAT_MODEL = openai/gpt-4.1

# Additional arguments for the LLM client (JSON object)
#ELISA_LLM_KWARGS = {"base_url": "..."}

# API key (provider specific env variable)
OPENAI_API_KEY = sk-...

# Strategies to check user messages (comma separated)
#  - "message_size":       Reject too big messages
#  - "content_safety_llm": Use the LLM to flag abusive messages
ELISA_GUARD_RAILS = message_size, content_safety_llm

# Maximum number of characters for user messages (guard rail "message_size")
ELISA_MAX_MSG_SIZE = 25000

# Strategy to route user messages to AI agents
#  - "default": Default implementation using the LLM chat completion client
ELISA_AGENT_ROUTER = default

# Maximum number of tries before falling back to the default agent
ELISA_AGENT_ROUTER_TRIES = 5

# Strategy to compress the conversation memory fed back to the LLM
#  - "default": Default implementation using the LLM chat completion client
ELISA_SUMMARIZER = default

# Strategy to suggest a title for new conversations
#  - "default": Default implementation using the LLM chat completion client
ELISA_TITLE_GENERATOR = default

#--------------------------------------------------------------------------------
# OpenID Connect Identity Provider
#--------------------------------------------------------------------------------

# JWKS URL of the identity provider (optional)
ELISA_OIDC_JWKS_URL = ...

# Public key for JWT decoding, instead of JWKS URL
#ELISA_OIDC_PUBLIC_KEY = ...

# Supported JWT signature algorithms
#ELISA_OIDC_ALGORITHMS = RS256

# Required claims in the JWT access token (comma separated)
#ELISA_OIDC_REQUIRE = aud, exp, scope

# Client ID of the server application
ELISA_OIDC_CLIENT_ID = ...

# Expected token issuer
ELISA_OIDC_ISSUER = ...

#--------------------------------------------------------------------------------
# Fallback values for development and demonstration without proper IdP. This sets
# a dummy user with fixed scopes (authorizations) that is always logged-in.
#--------------------------------------------------------------------------------

# Dummy user name
#ELISA_AUTH_SUBJECT = ...

# Fixed scopes for the dummy user (comma separated)
#ELISA_AUTH_SCOPES = chat_history